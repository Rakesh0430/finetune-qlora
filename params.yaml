TrainingArguments:
 num_train_epochs: 0.01
 per_device_train_batch_size: 4
 gradient_accumulation_steps: 1
 per_device_eval_batch_size: 4
 gradient_checkpointing: True
 optim: "paged_adamw_32bit"
 save_steps: 0
 logging_steps: 25
 learning_rate: 2e-4
 weight_decay: 0.001
 fp16: False
 bf16: False
 max_grad_norm: 0.3
 max_steps: -1
 warmup_ratio: 0.03
 group_by_length: True
 lr_scheduler_type: "cosine"
 report_to: "tensorboard"
 max_seq_length: None
 packing: False


LoraConfiguration:
 load_in_4bit: True
 bnb_4bit_quant_type: "nf4"
 bnb_4bit_compute_dtype: "float16"
 bnb_4bit_use_double_quant: False
 lora_alpha: 16
 lora_dropout: 0.1
 lora_r: 64
 bias: "none"
 task_type: "CAUSAL_LM"
#  use_nested_quant: False
#  use_4bit: True
